from unsloth import FastLanguageModel
import torch

# é‡æ–°åŠ è½½åˆšæ‰å¾®è°ƒå¥½çš„æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "llama3_financial_analyst_checkpoint", # è¿™é‡Œå¡«ä½ åˆšæ‰ä¿å­˜çš„è·¯å¾„
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model)

# å‡†å¤‡ä¸€æ¡æœ€æ–°çš„æ–°é—» (ä½ å¯ä»¥å» Google æœä¸€æ¡ä»Šå¤©çš„ NVDA æ–°é—»å¡«è¿›å»)
# æ¯”å¦‚ï¼šæ˜¨å¤© NVDA è·Œäº†ï¼Œæ–°é—»è¯´æ˜¯å› ä¸ºåå„æ–­è°ƒæŸ¥
news_headline = "NVIDIA has been granted permission to sell H200 chips to China, with 25% of the sales revenue going to the US government"
ticker = "NVDA"
date = "2025-12-09"

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Analyze the following financial news headline. Predict the stock movement (UP/DOWN) for the next trading day and provide a brief reasoning.

### Input:
Ticker: {}
Date: {}
Headline: {}

### Response:
"""

inputs = tokenizer(
    [alpaca_prompt.format(ticker, date, news_headline)], 
    return_tensors = "pt"
).to("cuda")

print("ğŸ¤– AI åˆ†æå¸ˆæ­£åœ¨æ€è€ƒ...")
outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)
result = tokenizer.batch_decode(outputs)[0]

# æå– Response åçš„å†…å®¹
print("\n" + "="*30)
print(result.split("### Response:")[-1].strip())
print("="*30)