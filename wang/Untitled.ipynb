{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989ca860-43d1-4c1b-bd58-0882a454e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•·ï¸ [1/3] å¯åŠ¨ Google News çˆ¬è™« (2023-10-23 - 2025-12-31)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [08:27<00:00, 72.57s/it]\n",
      "12/31/2025 06:47:11 PM - Failed to create TzCache, reason: Error creating TzCache folder: '/root/.cache/py-yfinance' reason: [Errno 17] File exists: '/root/.cache/py-yfinance'. TzCache will not be used. Tip: You can direct cache to use a different location with 'set_tz_cache_location(mylocation)'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> çˆ¬å–å®Œæˆï¼Œå…± 685 æ¡åŸå§‹æ–°é—»\n",
      "ğŸ“ˆ [2/3] ä¸‹è½½è‚¡ä»·å¹¶ç”Ÿæˆã€æ¶¨è·Œæ ‡ç­¾ã€‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  7 of 7 completed\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 685/685 [00:00<00:00, 2207.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼ä¿å­˜ä¸º: my_custom_fin_dataset_2025.csv\n",
      "   æœ‰æ•ˆæ ·æœ¬æ•°: 685\n",
      "   ç°åœ¨ä½ å¯ä»¥ç”¨è¿™ä¸ª CSV å»å–‚ç»™ Llama-3 äº†ï¼\n",
      "\n",
      "æ ·æœ¬é¢„è§ˆ:\n",
      "         Date Ticker                                           Headline  \\\n",
      "0  2025-12-29   NVDA  1 Incredible Reason to Buy Nvidia Stock Before...   \n",
      "1  2025-02-26   NVDA  NVIDIA Announces Financial Results for Fourth ...   \n",
      "2  2025-12-07   NVDA  Why Nvidia and Other AI Stocks Have Lost Their...   \n",
      "3  2025-12-05   NVDA  ChatGPT Thinks Nvidia Stock Price Will Close A...   \n",
      "4  2025-08-27   NVDA  NVIDIA Announces Financial Results for Second ...   \n",
      "\n",
      "     Label  \n",
      "0  NEUTRAL  \n",
      "1     DOWN  \n",
      "2  NEUTRAL  \n",
      "3       UP  \n",
      "4     DOWN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from gnews import GNews\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "from newspaper import Article\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "# æˆ‘ä»¬åªå…³æ³¨è¿™ä¸€æ¡â€œAI é»„é‡‘ä¾›åº”é“¾â€ï¼Œé€»è¾‘æœ€å¼º\n",
    "TARGET_TICKERS = [\"NVDA\", \"TSM\", \"AMD\", \"MSFT\", \"AAPL\", \"GOOGL\", \"META\"]\n",
    "\n",
    "# æ—¶é—´èŒƒå›´ (å»ºè®®çˆ¬è¿‡å» 1-2 å¹´çš„ï¼Œæ•°æ®é‡æ­£å¥½é€‚åˆå¾®è°ƒ)\n",
    "START_DATE = (datetime.datetime.now() - datetime.timedelta(days=800)).strftime(\"%Y-%m-%d\") # è¿‡å»ä¸€å¹´\n",
    "END_DATE = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "OUTPUT_FILE = \"my_custom_fin_dataset_2025.csv\"\n",
    "\n",
    "# ================= 1. æ–°é—»çˆ¬è™« (Google News) =================\n",
    "def fetch_google_news(tickers):\n",
    "    print(f\"ğŸ•·ï¸ [1/3] å¯åŠ¨ Google News çˆ¬è™« ({START_DATE} - {END_DATE})...\")\n",
    "    \n",
    "    google_news = GNews(language='en', country='US', period='2y', max_results=100) # è¿™é‡Œçš„ period å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    all_news = []\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # æœç´¢å…³é”®è¯ï¼šå…¬å¸è‚¡ç¥¨ä»£ç  + \"stock\" æˆ–è€… \"revenue\" ç­‰è¯ï¼Œæé«˜ç›¸å…³æ€§\n",
    "        keyword = f\"{ticker} stock news\"\n",
    "        json_resp = google_news.get_news(keyword)\n",
    "        \n",
    "        for article in json_resp:\n",
    "            # ç®€å•æ¸…æ´—\n",
    "            title = article.get('title', '')\n",
    "            pub_date = article.get('published date', '')\n",
    "            url = article.get('url', '')\n",
    "            \n",
    "            # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜\n",
    "            if len(title) < 10: continue\n",
    "            \n",
    "            # æ ¼å¼åŒ–æ—¥æœŸ (GNews è¿”å›çš„æ ¼å¼å¾ˆä¹±ï¼Œéœ€è¦ç»Ÿä¸€)\n",
    "            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå°è¯•è§£æï¼Œè§£æä¸äº†å°±è·³è¿‡\n",
    "            try:\n",
    "                # GNews æ ¼å¼é€šå¸¸æ˜¯: \"Fri, 27 Dec 2024 07:00:00 GMT\"\n",
    "                dt = pd.to_datetime(pub_date).strftime(\"%Y-%m-%d\")\n",
    "            except:\n",
    "                dt = datetime.datetime.now().strftime(\"%Y-%m-%d\") # å…œåº•\n",
    "                \n",
    "            all_news.append({\n",
    "                'Date': dt,\n",
    "                'Ticker': ticker,\n",
    "                'Headline': title,\n",
    "                'Source': article.get('publisher', {}).get('title', 'Unknown'),\n",
    "                'URL': url\n",
    "            })\n",
    "            \n",
    "        # ç¤¼è²Œå»¶æ—¶ï¼Œé˜²æ­¢ Google å° IP\n",
    "        time.sleep(2)\n",
    "        \n",
    "    df_news = pd.DataFrame(all_news)\n",
    "    # å»é‡ (ä¸åŒå…³é”®è¯å¯èƒ½çˆ¬åˆ°åŒä¸€æ¡æ–°é—»)\n",
    "    df_news = df_news.drop_duplicates(subset=['Headline'])\n",
    "    print(f\"   -> çˆ¬å–å®Œæˆï¼Œå…± {len(df_news)} æ¡åŸå§‹æ–°é—»\")\n",
    "    return df_news\n",
    "\n",
    "# ================= 2. è‚¡ä»·è·å–ä¸æ ‡ç­¾ç”Ÿæˆ =================\n",
    "def align_prices_and_label(news_df):\n",
    "    print(\"ğŸ“ˆ [2/3] ä¸‹è½½è‚¡ä»·å¹¶ç”Ÿæˆã€æ¶¨è·Œæ ‡ç­¾ã€‘...\")\n",
    "    \n",
    "    final_data = []\n",
    "    unique_tickers = news_df['Ticker'].unique()\n",
    "    \n",
    "    # æ‰¹é‡ä¸‹è½½è‚¡ä»·\n",
    "    prices = yf.download(list(unique_tickers), start=START_DATE, end=END_DATE)['Close']\n",
    "    \n",
    "    # è®¡ç®— T+1 æ”¶ç›Šç‡\n",
    "    # Shift(-1) å› ä¸ºæˆ‘ä»¬ç”¨ã€ä»Šå¤©çš„æ–°é—»ã€‘é¢„æµ‹ã€æ˜å¤©çš„æ¶¨è·Œã€‘\n",
    "    returns = prices.pct_change().shift(-1)\n",
    "    \n",
    "    for idx, row in tqdm(news_df.iterrows(), total=len(news_df)):\n",
    "        ticker = row['Ticker']\n",
    "        date_str = row['Date']\n",
    "        \n",
    "        try:\n",
    "            # æŸ¥æ‰¾å½“å¤©çš„æ”¶ç›Šç‡\n",
    "            # æ³¨æ„ï¼šå¦‚æœæ–°é—»æ˜¯å‘¨æœ«å‘çš„ï¼Œæˆ‘ä»¬è¦å¯¹åº”åˆ°å‘¨ä¸€çš„æ”¶ç›Šç‡\n",
    "            # è¿™é‡Œç”¨ asof æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥\n",
    "            dt = pd.to_datetime(date_str)\n",
    "            if dt not in returns.index:\n",
    "                # æ‰¾æœ€è¿‘çš„ä¸€ä¸ªæœªæ¥äº¤æ˜“æ—¥\n",
    "                valid_dates = returns.index[returns.index > dt]\n",
    "                if len(valid_dates) == 0: continue\n",
    "                target_date = valid_dates[0]\n",
    "            else:\n",
    "                target_date = dt\n",
    "                \n",
    "            ret = returns.loc[target_date][ticker]\n",
    "            \n",
    "            # æ‰“æ ‡ç­¾ (Labeling)\n",
    "            # æ¶¨ (UP): > 0.5%\n",
    "            # è·Œ (DOWN): < -0.5%\n",
    "            # éœ‡è¡ (NEUTRAL): -0.5% ~ 0.5%\n",
    "            \n",
    "            label = \"NEUTRAL\"\n",
    "            if ret > 0.005: label = \"UP\"\n",
    "            elif ret < -0.005: label = \"DOWN\"\n",
    "            \n",
    "            # åªæœ‰æ˜ç¡®çš„æ¶¨è·Œæ‰é€‚åˆè®­ç»ƒ LLMï¼Œéœ‡è¡æ•°æ®å¯èƒ½ä¼šé€ æˆæ··æ·†\n",
    "            # ç­–ç•¥ï¼šä¿ç•™éœ‡è¡æ•°æ®ä½†æ ‡è®°ï¼Œæˆ–è€…ç›´æ¥ä¸¢å¼ƒã€‚ä¸ºäº†è®­ç»ƒæ•ˆæœï¼Œå»ºè®®ä¿ç•™æˆ–ä¸¢å¼ƒã€‚\n",
    "            \n",
    "            # æ„å»º CoT (æ€ç»´é“¾) è®­ç»ƒæ•°æ®æ‰€éœ€çš„ \"Reasoning\" (ä¼ªé€ /æ¨æ–­)\n",
    "            # æ³¨æ„ï¼šåœ¨çœŸå®å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬ä¼šè®© LLM è‡ªå·±å­¦ reasoningï¼Œ\n",
    "            # è¿™é‡Œæˆ‘ä»¬å…ˆå‡†å¤‡å¥½ Input (Headline) å’Œ Output (Label)\n",
    "            \n",
    "            final_data.append({\n",
    "                'Date': date_str,\n",
    "                'Ticker': ticker,\n",
    "                'Headline': row['Headline'],\n",
    "                'Source': row['Source'],\n",
    "                'Next_Day_Return': ret,\n",
    "                'Label': label\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    df_final = pd.DataFrame(final_data)\n",
    "    # è¿‡æ»¤æ‰ Neutral (å¯é€‰ï¼Œå¦‚æœä½ æƒ³åšäºŒåˆ†ç±»)\n",
    "    # df_final = df_final[df_final['Label'] != \"NEUTRAL\"]\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# ================= 3. ä¸»ç¨‹åº =================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. çˆ¬æ–°é—»\n",
    "    news_df = fetch_google_news(TARGET_TICKERS)\n",
    "    \n",
    "    if not news_df.empty:\n",
    "        # 2. å¯¹é½è‚¡ä»·\n",
    "        dataset = align_prices_and_label(news_df)\n",
    "        \n",
    "        # 3. ä¿å­˜\n",
    "        dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼ä¿å­˜ä¸º: {OUTPUT_FILE}\")\n",
    "        print(f\"   æœ‰æ•ˆæ ·æœ¬æ•°: {len(dataset)}\")\n",
    "        print(\"   ç°åœ¨ä½ å¯ä»¥ç”¨è¿™ä¸ª CSV å»å–‚ç»™ Llama-3 äº†ï¼\")\n",
    "        print(\"\\næ ·æœ¬é¢„è§ˆ:\")\n",
    "        print(dataset[['Date', 'Ticker', 'Headline', 'Label']].head())\n",
    "    else:\n",
    "        print(\"âŒ æ²¡çˆ¬åˆ°æ–°é—»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ (å¯èƒ½éœ€è¦æ¢¯å­è¿æ¥ Google)ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52113b-44c1-47d9-9454-6efe157b7264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
